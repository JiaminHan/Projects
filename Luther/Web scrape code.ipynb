{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url ='https://flightaware.com/live/findflight?origin=KSEA&destination=KLAX'\n",
    "response = requests.get(url)\n",
    "page = response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from datetime import datetime \n",
    "import os\n",
    "import dateutil.parser\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "chromedriver = \"chromedriver.exe\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "#driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "#driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "options= Options()\n",
    "#options.add_argument(\"--headless\") # Runs Chrome in headless mode.\n",
    "options.add_argument('--allow-no-sandbox-job') # Bypass OS security model\n",
    "options.add_argument(\"--test-type\")\n",
    "options.add_argument('--disable-gpu')  # applicable to windows os only\n",
    "options.add_argument('start-maximized') # \n",
    "options.add_argument('disable-infobars')\n",
    "#options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument(\"--disable-web-security\")\n",
    "options.add_argument(\"--allow-running-insecure-content\")\n",
    "options.add_argument(\"--safebrowsing-disable-extension-blacklist\")\n",
    "options.add_argument(\"--safebrowsing-disable-download-protection\")\n",
    "options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "options.add_argument('--disable-bundled-ppapi-flash')\n",
    "#options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\")\n",
    "\n",
    "#PROXY = \"185.21.156.3:41258\" # IP:PORT or HOST:PORT\n",
    "#options.add_argument('--proxy-server=%s' % PROXY)\n",
    "\n",
    "\n",
    "#options.add_extension(\"C:/Users/Zaixing/Projects/Luther/Adblock-Plus_v3.1.crx\")\n",
    "options.add_extension(\"Adblock-Plus_v3.1.crx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chromedriver,chrome_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.whoishostingthis.com/tools/user-agent/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flightaware.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log in\n",
    "driver.find_element_by_xpath('//*[@id=\"topWrapper\"]/div[1]/nav/div/div[2]/a').click()\n",
    "driver.find_element_by_xpath('//*[@id=\"popupLogin\"]/div/div[2]/div/form/div[1]/input[1]').send_keys('johnlovesflying')\n",
    "driver.find_element_by_xpath('//*[@id=\"popupLogin\"]/div/div[2]/div/form/div[1]/input[2]').send_keys('2014852')\n",
    "driver.find_element_by_xpath('//*[@id=\"popupLogin\"]/div/div[2]/div/form/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_list=['KLAX','KSFO','PANC','KDEN','KLAS','KPHX','KORD','KPDX','KDFW','KSJC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate flightid\n",
    "def get_flight(x):\n",
    "    driver.get(x)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    len_flight=len(soup.find_all('table')[1].find_all('tr'))\n",
    "    row_num=list(range(1,len_flight,2))\n",
    "    la_flight=[]\n",
    "    for i in row_num:\n",
    "        fl=soup.find_all('table')[1].find_all('tr')[i].find_all('td')[1].text.strip()\n",
    "        la_flight.append(fl)\n",
    "    la_flight_uni = sorted(list(set(la_flight)))\n",
    "\n",
    "    return la_flight_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all unique flightid for destination airports above\n",
    "city_url_list=[]\n",
    "for i in destination_list:\n",
    "    temp_list = get_flight(\"https://flightaware.com/live/findflight?origin=KSEA&destination=\"+i)\n",
    "    city_url_list.append(temp_list)\n",
    "    time.sleep(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all the flightid into one list\n",
    "import itertools\n",
    "flightid_list=list(itertools.chain.from_iterable(city_url_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle master flightid list\n",
    "import pickle\n",
    "with open('master_flightid_list.pickle', 'wb') as f:\n",
    "    pickle.dump(flightid_list, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following steps are to generate a list of all flights during the past three months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://flightaware.com/live/flight/ABX1817/history/160')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a master list of all url for each flight each day\n",
    "url_all_flight=[]\n",
    "for i in flightid_list:\n",
    "    driver.get('https://flightaware.com/live/flight/'+i+'/history/160')\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    table=soup.find(class_='prettyTable fullWidth tablesaw tablesaw-stack')\n",
    "    rows=[row for row in table.find_all('a')]\n",
    "    url_part=rows[0:len(rows):3]\n",
    "    \n",
    "    url_per_flight=[]\n",
    "    for j in url_part:\n",
    "        if 'Z/KSEA'in str(j):\n",
    "            j= str(j).split('>')[0].replace(\"<a href=\",'').replace('\"','')\n",
    "            url_per_flight.append('https://flightaware.com'+j)\n",
    "    time.sleep(8)\n",
    "    url_all_flight.append(url_per_flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_all_flight=list(itertools.chain.from_iterable(url_all_flight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle master list url\n",
    "with open('master_url_list.pickle', 'wb') as f:\n",
    "    pickle.dump(url_all_flight, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following steps are to scrape info from each web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_class(s,c):\n",
    "    try:\n",
    "        return s.find_all(class_=c)[0].text.strip()\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_info_page(url):\n",
    "\n",
    "    #driver.get(url)\n",
    "\n",
    "    #soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    soup = BeautifulSoup(url, 'html.parser')\n",
    "    data={}\n",
    "    #find airline\n",
    "    ##airline=soup.find('title').text.split()[0]\n",
    "    airline=soup.find(class_=\"flightPageAirlineCallsign\").text.strip()\n",
    "    \n",
    "    #find flightid\n",
    "    ##flightid=soup.find('title').text.split()[2].replace('(','').replace(')','')+soup.find('title').text.split()[3].replace('#','')\n",
    "    flightid=soup.find(class_=\"flightPageIdent\").text.strip().split('/')[0]\n",
    "    \n",
    "    #find flight date\n",
    "    ##date=dateutil.parser.parse(soup.find('title').text.split()[5])\n",
    "    date=dateutil.parser.parse(soup.find(class_='flightPageSummaryDepartureDay').text.strip().split(' ')[1])\n",
    "    \n",
    "    #find destination airport\n",
    "    ##destination=soup.find('title').text.split()[9]\n",
    "    destination=soup.find(class_=\"flightPageSummaryDestination\").findChildren()[0].text.strip()\n",
    "    \n",
    "    #find departure gate\n",
    "    ##gate=soup.find_all('strong')[3].text.split()[1]\n",
    "    gate=find_class(soup,\"flightPageAirportGate\")\n",
    "    \n",
    "    # identify flight time data structure\n",
    "    time_type = [i for i in soup.find_all(class_=\"flightPageDataTableHeading\") if ('Departure Times' in i.text or 'Flight Times' in i.text)][0].text.strip()\n",
    "    \n",
    "    #find departure times based on data structure\n",
    "    if time_type=='Departure Times':\n",
    "        #find scheduled  Departure time\n",
    "        scheduled_departure=soup.find_all(text=\"Scheduled \")[0].findNextSibling().text.strip()\n",
    "    \n",
    "        #find actual departure time\n",
    "        actual_departure=soup.find_all(class_='flightPageDataActualTimeText')[0].text.strip()\n",
    "\n",
    "        #find scheduled  takeoff time\n",
    "        scheduled_takeoff=soup.find_all(text=\"Scheduled \")[0].findNextSibling().text.strip()\n",
    "        \n",
    "        #find actual takeoff time\n",
    "        actual_takeoff=soup.find_all(class_='flightPageDataActualTimeText')[1].text.strip()\n",
    "\n",
    "        \n",
    "    if time_type=='Flight Times':\n",
    "        #find scheduled  Departure time\n",
    "        scheduled_departure=None\n",
    "    \n",
    "        #find actual departure time\n",
    "        actual_departure=None\n",
    "\n",
    "        #find scheduled  Departure time\n",
    "        scheduled_takeoff=soup.find_all(class_='flightPageDataAncillaryText')[0].text.strip().split('\\n')[1]\n",
    "        \n",
    "        #find actual takeoff time\n",
    "        actual_takeoff=soup.find_all(class_='flightPageDataActualTimeText')[0].text.strip()\n",
    "\n",
    "    \n",
    "    #find aircraft type\n",
    "    aircraft=soup.find(class_='flightPageData').text.strip().split('\\xa0')[0]\n",
    "    \n",
    "    #find taxi time\n",
    "    #taxi_time=soup.find(text=\"Taxi Time: \").findNextSibling().text.strip()\n",
    "    \n",
    "    #find delay status\n",
    "    delay=soup.find(class_='flightPageOriginDelayStatus').text.strip().replace('(','').replace(')','')\n",
    "    \n",
    "    #find distance\n",
    "    distance =''.join([i.text.strip() for i in soup.find_all(class_='flightPageData') if ('Actual:' in i.text or 'Planned:' in i.text or'Direct:' in i.text)])\n",
    "    \n",
    "    data[date]=[date,airline,aircraft,flightid,destination,gate,distance,\n",
    "                scheduled_departure,actual_departure,scheduled_takeoff,actual_takeoff,delay]\n",
    "    df=pd.DataFrame(data).T\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('master_url_list.pickle', 'rb') as f:\n",
    "    url_all_flight=pickle.load(f)\n",
    "len(url_all_flight)\n",
    "\n",
    "with open('soup_bowl.pickle', 'rb') as f:\n",
    "    soup_bowl=pickle.load(f)\n",
    "len(soup_bowl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select URLS not have been scraped\n",
    "from random import shuffle\n",
    "\n",
    "with open('url_toscrape.pickle', 'rb') as f:\n",
    "    url_toscrape=pickle.load(f)\n",
    "url_toscrape_list = list(url_toscrape.sample(frac=1)['url'])\n",
    "len(url_toscrape_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start scraping!\n",
    "\n",
    "#all_flight=[]\n",
    "#soup_bowl = []\n",
    "success_url = []\n",
    "fail_url = []\n",
    "n=1\n",
    "\n",
    "for i in tqdm(url_toscrape_list[104:]):\n",
    "    user_agent=random.choice(user_agent_list) # pick a user agent\n",
    "    #PROXY = random.choice(useable_proxies[0:10]) # pick a proxy\n",
    "    options.add_argument(\"user-agent=\"+user_agent)\n",
    "    #options.add_argument('--proxy-server=%s' % PROXY)\n",
    "    \n",
    "    try:\n",
    "        driver.get(i)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        soup_bowl.append(str(soup))\n",
    "        #all_flight.append(get_info_page(i))\n",
    "        success_url.append(i)\n",
    "    except:\n",
    "        fail_url.append(i)\n",
    "    \n",
    "    n+=1\n",
    "    if n==50:\n",
    "        with open('soup_bowl.pickle', 'wb') as fp:\n",
    "            pickle.dump(soup_bowl, fp)\n",
    "        n=1\n",
    " \n",
    "    #result = pd.concat(all_flight)\n",
    "\n",
    "        \n",
    "    time.sleep(random.sample(range(9),1)[0])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup_bowl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(success_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fail_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5980"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_url = [i for i in url_toscrape['url'] if i not in success_url]\n",
    "len(remaining_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-8cf27f2784e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_info_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup_bowl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-82-8cf27f2784e4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_info_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup_bowl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-4d8c48b84486>\u001b[0m in \u001b[0;36mget_info_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#find airline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m##airline=soup.find('title').text.split()[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mairline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"flightPageAirlineCallsign\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#find flightid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "result = pd.concat([get_info_page(i) for i in soup_bowl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "driver.get('https://flightaware.com/live/flight/ASA348/history/20180625/0353Z/KSEA/KSJC')\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# scheduled_departure=soup.find_all(text=\"Scheduled \")[1].findNextSibling().text.strip()\n",
    "# actual_departure=soup.find_all(class_='flightPageDataActualTimeText')[1].text.strip()\n",
    "#print(soup.find('div', {'class': 'flightPageDataActualTimeText'}).findChildren())\n",
    "soup=BeautifulSoup(str(soup), 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 6, 24, 0, 0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateutil.parser.parse(soup.find(class_='flightPageSummaryDepartureDay').text.strip().split(' ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"left Gate N9\\n\\n\\nSeattle-Tacoma Intl - \\nSEA\\n\\n\\n\\n\\n\\n\\narrived at \\xa0Gate 28\\n\\n\\n\\nSan Jose Int'l - \\nSJC\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_='flightPageAirportGates')[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use proxy in requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://httpbin.org/ip'\n",
    "proxies = {\n",
    "    \"http\": 'http://5.101.203.50:8080', \n",
    "    \"https\": 'http://5.101.203.50:8080'\n",
    "}\n",
    "response = requests.get(url,proxies=proxies)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml.html import fromstring\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:20]:\n",
    "        if i.xpath('.//td[3][contains(text(),\"US\")]'):\n",
    "            #Grabbing IP and corresponding PORT\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = get_proxies()\n",
    "print(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback\n",
    "#If you are copy pasting proxy ips, put in the list below\n",
    "#proxies = ['121.129.127.209:80', '124.41.215.238:45169', '185.93.3.123:8080', '194.182.64.67:3128', '106.0.38.174:8080', '163.172.175.210:3128', '13.92.196.150:8080']\n",
    "proxies = get_proxies()\n",
    "proxy_pool = cycle(proxies)\n",
    "useable_proxies = []\n",
    "\n",
    "url = 'https://httpbin.org/ip'\n",
    "for i in range(1,31):\n",
    "    #Get a proxy from the pool\n",
    "    proxy = next(proxy_pool)\n",
    "    print(\"Request #%d\"%i)\n",
    "    try:\n",
    "        response.time = requests.get(url,proxies={\"http\": proxy, \"https\": proxy}).elapsed.total_seconds()\n",
    "        if response.time<=4:\n",
    "            useable_proxies.append(proxy)\n",
    "        #print(response.time)\n",
    "        #useable_proxies = useable_proxies.append(proxy)\n",
    "    except:\n",
    "        #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "        #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "        print(\"Skipping. Connnection error\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(list(proxies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useable_proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useable_proxies = []\n",
    "useable_proxies.append(next(proxy_pool))\n",
    "print(useable_proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useable_proxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hide user agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = 'https://httpbin.org/user-agent'\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'\n",
    "request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "response = urllib.request.urlopen(request)\n",
    "html = response.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request #1\n",
      "User-Agent Sent:Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko\"}\\n'\n",
      "-------------------\n",
      "\n",
      "\n",
      "Request #2\n",
      "User-Agent Sent:Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\"}\\n'\n",
      "-------------------\n",
      "\n",
      "\n",
      "Request #3\n",
      "User-Agent Sent:Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\"}\\n'\n",
      "-------------------\n",
      "\n",
      "\n",
      "Request #4\n",
      "User-Agent Sent:Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\"}\\n'\n",
      "-------------------\n",
      "\n",
      "\n",
      "Request #5\n",
      "User-Agent Sent:Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\n",
      "User Agent Recevied by HTTPBin:\n",
      "b'{\"user-agent\":\"Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\"}\\n'\n",
      "-------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import random\n",
    "user_agent_list = [\n",
    "   #Chrome\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    #Firefox\n",
    "    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
    "]\n",
    "url = 'https://httpbin.org/user-agent'\n",
    "#Lets make 5 requests and see what user agents are used \n",
    "for i in range(1,6):\n",
    "    #Pick a random user agent\n",
    "    user_agent = random.choice(user_agent_list)\n",
    "    #Set the headers \n",
    "    headers = {'User-Agent': user_agent}\n",
    "    #Make the request\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
    "    response = urllib.request.urlopen(request)\n",
    "    html = response.read()\n",
    "    \n",
    "    print(\"Request #%d\\nUser-Agent Sent:%s\\nUser Agent Recevied by HTTPBin:\"%(i,user_agent))\n",
    "    print(html)\n",
    "    print(\"-------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(user_agent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(useable_proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
